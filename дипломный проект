{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPwY0UWGKFdUAGv024Y/a96",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MylayMeretova/-/blob/main/%D0%B4%D0%B8%D0%BF%D0%BB%D0%BE%D0%BC%D0%BD%D1%8B%D0%B9%20%D0%BF%D1%80%D0%BE%D0%B5%D0%BA%D1%82\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import files\n",
        "from io import BytesIO\n",
        "from PIL import Image\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "\n",
        "upl = files.upload()\n",
        "img = Image.open(BytesIO(upl['img.jpg']))\n",
        "img_style = Image.open(BytesIO(upl['img_style.jpg']))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow( img )\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow( img_style )\n",
        "plt.show()\n",
        "\n",
        "x_img = keras.applications.vgg19.preprocess_input( np.expand_dims(img, axis=0) )\n",
        "x_style = keras.applications.vgg19.preprocess_input(np.expand_dims(img_style, axis=0))\n",
        "\n",
        "def deprocess_img(processed_img):\n",
        "  x = processed_img.copy()\n",
        "  if len(x.shape) == 4:\n",
        "    x = np.squeeze(x, 0)\n",
        "  assert len(x.shape) == 3, (\"Input to deprocess image must be an image of \"\n",
        "                             \"dimension [1, height, width, channel] or [height, width, channel]\")\n",
        "  if len(x.shape) != 3:\n",
        "    raise ValueError(\"Invalid input to deprocessing image\")\n",
        "  \n",
        "  # perform the inverse of the preprocessing step\n",
        "  x[:, :, 0] += 103.939\n",
        "  x[:, :, 1] += 116.779\n",
        "  x[:, :, 2] += 123.68\n",
        "  x = x[:, :, ::-1]\n",
        "\n",
        "  x = np.clip(x, 0, 255).astype('uint8')\n",
        "  return x\n",
        "\n",
        "vgg = keras.applications.vgg19.VGG19(include_top=False, weights='imagenet')\n",
        "vgg.trainable = False \n",
        "\n",
        "# Content layer where will pull our feature maps\n",
        "content_layers = ['block5_conv2'] \n",
        "\n",
        "# Style layer we are interested in\n",
        "style_layers = ['block1_conv1',\n",
        "                'block2_conv1',\n",
        "                'block3_conv1', \n",
        "                'block4_conv1', \n",
        "                'block5_conv1'\n",
        "               ]\n",
        "\n",
        "num_content_layers = len(content_layers)\n",
        "num_style_layers = len(style_layers)\n",
        "\n",
        "style_outputs = [vgg.get_layer(name).output for name in style_layers]\n",
        "content_outputs = [vgg.get_layer(name).output for name in content_layers]\n",
        "model_outputs = style_outputs + content_outputs\n",
        "\n",
        "print(vgg.input)\n",
        "for m in model_outputs:\n",
        "  print(m)\n",
        "\n",
        "model = keras.models.Model(vgg.input, model_outputs)\n",
        "for layer in model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "print(model.summary())      # вывод структуры НС в консоль\n",
        "\n",
        "def get_feature_representations(model):\n",
        "  # batch compute content and style features\n",
        "  style_outputs = model(x_style)\n",
        "  content_outputs = model(x_img)\n",
        "  \n",
        "  # Get the style and content feature representations from our model  \n",
        "  style_features = [style_layer[0] for style_layer in style_outputs[:num_style_layers]]\n",
        "  content_features = [content_layer[0] for content_layer in content_outputs[num_style_layers:]]\n",
        "  return style_features, content_features\n",
        "\n",
        "def get_content_loss(base_content, target):\n",
        "  return tf.reduce_mean(tf.square(base_content - target))\n",
        "\n",
        "def gram_matrix(input_tensor):\n",
        "  # We make the image channels first \n",
        "  channels = int(input_tensor.shape[-1])\n",
        "  a = tf.reshape(input_tensor, [-1, channels])\n",
        "  n = tf.shape(a)[0]\n",
        "  gram = tf.matmul(a, a, transpose_a=True)\n",
        "  return gram / tf.cast(n, tf.float32)\n",
        "\n",
        "def get_style_loss(base_style, gram_target):\n",
        "  gram_style = gram_matrix(base_style)\n",
        "  \n",
        "  return tf.reduce_mean(tf.square(gram_style - gram_target))\n",
        "\n",
        "def compute_loss(model, loss_weights, init_image, gram_style_features, content_features):\n",
        "  style_weight, content_weight = loss_weights\n",
        "  \n",
        "  model_outputs = model(init_image)\n",
        "  \n",
        "  style_output_features = model_outputs[:num_style_layers]\n",
        "  content_output_features = model_outputs[num_style_layers:]\n",
        "  \n",
        "  style_score = 0\n",
        "  content_score = 0\n",
        "\n",
        "  # Accumulate style losses from all layers\n",
        "  # Here, we equally weight each contribution of each loss layer\n",
        "  weight_per_style_layer = 1.0 / float(num_style_layers)\n",
        "  for target_style, comb_style in zip(gram_style_features, style_output_features):\n",
        "    style_score += weight_per_style_layer * get_style_loss(comb_style[0], target_style)\n",
        "    \n",
        "  # Accumulate content losses from all layers \n",
        "  weight_per_content_layer = 1.0 / float(num_content_layers)\n",
        "  for target_content, comb_content in zip(content_features, content_output_features):\n",
        "    content_score += weight_per_content_layer* get_content_loss(comb_content[0], target_content)\n",
        "  \n",
        "  style_score *= style_weight\n",
        "  content_score *= content_weight\n",
        "\n",
        "  # Get total loss\n",
        "  loss = style_score + content_score \n",
        "  return loss, style_score, content_score\n",
        "\n",
        "num_iterations=100\n",
        "content_weight=1e3\n",
        "style_weight=1e-2\n",
        "\n",
        "style_features, content_features = get_feature_representations(model)\n",
        "gram_style_features = [gram_matrix(style_feature) for style_feature in style_features]\n",
        "\n",
        "init_image = np.copy(x_img)\n",
        "init_image = tf.Variable(init_image, dtype=tf.float32)\n",
        "\n",
        "opt = tf.compat.v1.train.AdamOptimizer(learning_rate=2, beta1=0.99, epsilon=1e-1)\n",
        "iter_count = 1\n",
        "best_loss, best_img = float('inf'), None\n",
        "loss_weights = (style_weight, content_weight)\n",
        "\n",
        "cfg = {\n",
        "      'model': model,\n",
        "      'loss_weights': loss_weights,\n",
        "      'init_image': init_image,\n",
        "      'gram_style_features': gram_style_features,\n",
        "      'content_features': content_features\n",
        "}\n",
        "\n",
        "norm_means = np.array([103.939, 116.779, 123.68])\n",
        "min_vals = -norm_means\n",
        "max_vals = 255 - norm_means\n",
        "imgs = []\n",
        "\n",
        "for i in range(num_iterations):\n",
        "    with tf.GradientTape() as tape: \n",
        "       all_loss = compute_loss(**cfg)\n",
        "    \n",
        "    loss, style_score, content_score = all_loss\n",
        "    grads = tape.gradient(loss, init_image)\n",
        "\n",
        "    opt.apply_gradients([(grads, init_image)])\n",
        "    clipped = tf.clip_by_value(init_image, min_vals, max_vals)\n",
        "    init_image.assign(clipped)\n",
        "    \n",
        "    if loss < best_loss:\n",
        "      # Update best loss and best image from total loss. \n",
        "      best_loss = loss\n",
        "      best_img = deprocess_img(init_image.numpy())\n",
        "\n",
        "      # Use the .numpy() method to get the concrete numpy array\n",
        "      plot_img = deprocess_img(init_image.numpy())\n",
        "      imgs.append(plot_img)\n",
        "      print('Iteration: {}'.format(i))\n",
        "\n",
        "plt.imshow(best_img)\n",
        "print(best_loss)\n",
        "\n",
        "image = Image.fromarray(best_img.astype('uint8'), 'RGB')\n",
        "image.save(\"result.jpg\")\n",
        "files.download(\"result.jpg\")"
      ],
      "metadata": {
        "id": "JXCCuQqEOwyi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}